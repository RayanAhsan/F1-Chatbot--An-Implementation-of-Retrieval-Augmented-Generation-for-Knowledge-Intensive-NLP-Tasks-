# F1-Chatbot--An-Implementation-of-Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks-


## ğŸ“Œ Overview  
This project implements a **Retrieval-Augmented Generation (RAG)** chatbot fine-tuned on a **Formula 1 Q&A dataset**. This paper is an exemplar implementation of the seminal paper "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" which can be found here:
https://arxiv.org/abs/2005.11401

**Note**: This is not an EXACT replica of the research paper implementation:
âš¡ Compute limitations â€“ Training was conducted on a smaller GPU setup, limiting model size, retrieval depth, and training epochs.
ğŸ§© Simplified retriever â€“ A basic FAISS retriever was implemented, rather than a fully optimized dense retriever with large-scale pretrain

By combining **retrieval-based search** (via FAISS index of Formula 1 Wikipedia passages) with **generative modeling** (RAG-Sequence + BART), the chatbot provides accurate and context-rich answers to Formula 1-related questions.  

---

## âœ¨ Features  
- ğŸ” **FAISS Retriever** â€“ Retrieves top-k relevant Formula 1 passages from Wikipedia.  
- ğŸ¤– **RAG-Sequence Model** â€“ Fine-tuned with Formula 1 Q&A pairs for domain-specific accuracy.  
- ğŸ’¬ **Interactive Chatbot** â€“ Answer Formula 1 questions via command line interface.  
- ğŸ“Š **Evaluation Metrics** â€“ Measures performance using EM, F1, and ROUGE.  

---

## âš™ï¸ Tech Stack  
- **Model:** `facebook/rag-sequence-nq` (fine-tuned)  
- **Tokenizer:** `RagTokenizer`, `BartTokenizer`  
- **Retriever:** FAISS-based document index  
- **Training Framework:** Hugging Face `Seq2SeqTrainer`  
- **Language:** Python  

---

## Evaluation Results 

| Metric               | Score |
| -------------------- | ----- |
| **Exact Match (EM)** | 73.0% |
| **F1 Score**         | 82.0% |

